# Mutex to Atomic Conversion Analysis

## Executive Summary

This document analyzes all mutex usage in the codebase to identify candidates for conversion to atomic operations. The goal is to improve scalability by reducing lock contention, particularly in performance-sensitive paths.

**Key Findings:**
- **7 Easy conversions** (simple counters/single values)
- **3 Medium conversions** (structs with atomic.Value)
- **8 Keep as mutex** (complex state, maps, or low contention)

**Estimated Impact:**
- **High-value**: `client_stats.go` mutexes (called per-event, high frequency)
- **Medium-value**: `aggregator.go` snapshot mutexes (called per-TUI tick)
- **Low-value**: Test mutexes, low-frequency operations

---

## Analysis Methodology

### Categorization Criteria

1. **Easy (‚≠ê)**: Single value or simple counter ‚Üí `atomic.Int64`, `atomic.Float64`, etc.
2. **Medium (‚≠ê‚≠ê)**: Struct with multiple fields ‚Üí `atomic.Value` (pointer swap)
3. **Hard/Keep (‚ùå)**: Maps, complex state, or low contention ‚Üí Keep mutex

### Performance Sensitivity

- **High**: Called per-event (segment download, HTTP request)
- **Medium**: Called per-TUI tick (~2/sec) or per-aggregation
- **Low**: Called rarely (startup, shutdown, test code)

---

## Detailed Analysis by File

### 1. `internal/stats/client_stats.go` - HIGH PRIORITY ‚ö°

**Context**: Per-client statistics, called on every FFmpeg event. This is the **most performance-sensitive** code path.

#### 1.1 `bytesMu` - ‚≠ê EASY

**Current Usage:**
```go
bytesFromPreviousRuns int64
currentProcessBytes   int64
bytesMu               sync.Mutex

func (s *ClientStats) OnProcessStart() {
    s.bytesMu.Lock()
    s.bytesFromPreviousRuns += s.currentProcessBytes
    s.currentProcessBytes = 0
    s.bytesMu.Unlock()
}

func (s *ClientStats) UpdateCurrentBytes(totalSize int64) {
    s.bytesMu.Lock()
    s.currentProcessBytes = totalSize
    s.bytesMu.Unlock()
}

func (s *ClientStats) TotalBytes() int64 {
    s.bytesMu.Lock()
    defer s.bytesMu.Unlock()
    return s.bytesFromPreviousRuns + s.currentProcessBytes
}
```

**Analysis:**
- **Frequency**: High (called on every progress update)
- **Contention**: Medium (one mutex per client, but many concurrent clients)
- **Complexity**: Low (two int64 values)

**Conversion Strategy:**
```go
// Use atomic operations for both fields
bytesFromPreviousRuns atomic.Int64
currentProcessBytes   atomic.Int64

func (s *ClientStats) OnProcessStart() {
    // Atomic read-modify-write
    prev := s.currentProcessBytes.Load()
    s.currentProcessBytes.Store(0)
    s.bytesFromPreviousRuns.Add(prev)
}

func (s *ClientStats) UpdateCurrentBytes(totalSize int64) {
    s.currentProcessBytes.Store(totalSize)
}

func (s *ClientStats) TotalBytes() int64 {
    return s.bytesFromPreviousRuns.Load() + s.currentProcessBytes.Load()
}
```

**Benefits:**
- ‚úÖ Lock-free reads (most common operation)
- ‚úÖ Lock-free writes (UpdateCurrentBytes)
- ‚úÖ Only OnProcessStart needs atomic read-modify-write (rare)

**Difficulty**: ‚≠ê Easy (10 LOC changes)

**Recommendation**: ‚úÖ **CONVERT** - High value, easy implementation

---

#### 1.2 `httpErrorsMu` - ‚ùå KEEP

**Current Usage:**
```go
HTTPErrors   map[int]int64
httpErrorsMu sync.Mutex

func (s *ClientStats) RecordHTTPError(code int) {
    s.httpErrorsMu.Lock()
    s.HTTPErrors[code]++
    s.httpErrorsMu.Unlock()
}
```

**Analysis:**
- **Frequency**: Medium (only on HTTP errors, not every request)
- **Complexity**: High (map operations can't be atomic)
- **Alternatives**: `sync.Map` (but adds complexity, may not be faster)

**Conversion Strategy:**
- ‚ùå Cannot use atomics (map operations)
- Could use `sync.Map`, but:
  - More complex API
  - May not be faster for small maps
  - Current mutex is fine for low-frequency operations

**Recommendation**: ‚ùå **KEEP** - Map operations require mutex

---

#### 1.3 `inferredLatencyMu` - üóëÔ∏è **REMOVE** (Obsolete)

**Current Usage:**
```go
inferredLatencyDigest *tdigest.TDigest
inferredLatencyCount  int64
inferredLatencySum    time.Duration
inferredLatencyMax    time.Duration
inferredLatencyMu     sync.Mutex

func (s *ClientStats) recordInferredLatency(d time.Duration) {
    s.inferredLatencyMu.Lock()
    s.inferredLatencyDigest.Add(float64(d.Nanoseconds()), 1)
    s.inferredLatencyCount++
    s.inferredLatencySum += d
    if d > s.inferredLatencyMax {
        s.inferredLatencyMax = d
    }
    s.inferredLatencyMu.Unlock()
}
```

**Analysis:**
- **Frequency**: High (called per segment completion)
- **Complexity**: Very High (TDigest.Add() is not thread-safe, requires mutex)
- **Status**: **OBSOLETE** - Replaced by accurate timestamps in DebugEventParser

**Why Remove:**
- ‚úÖ **Inferred latency is obsolete** - We now have accurate segment wall time from FFmpeg timestamps
- ‚úÖ **Redundant system** - DebugEventParser already tracks accurate segment download times
- ‚úÖ **Less accurate** - Inferred from progress updates (imprecise) vs. FFmpeg timestamps (millisecond precision)
- ‚úÖ **Removes mutex** - One less lock to contend with
- ‚úÖ **Cleaner code** - Single source of truth for latency metrics

**Migration Strategy:**
1. Add T-Digest to `DebugEventParser` for percentile calculation (using accurate timestamps)
2. Calculate P50, P95, P99 from accurate segment wall times
3. Update TUI to use `DebugStats.SegmentWallTimeP50/P95/P99` instead of `InferredLatency*`
4. Remove inferred latency code entirely

**See**: `docs/REMOVE_INFERRED_LATENCY_ANALYSIS.md` for detailed migration plan

**Recommendation**: üóëÔ∏è **REMOVE** - Obsolete, replaced by accurate timestamps. See removal analysis document.

---

#### 1.4 `segmentSizeMu` - ‚≠ê‚≠ê MEDIUM

**Current Usage:**
```go
lastTotalSize  int64
segmentSizes   []int64
segmentSizeIdx int
segmentSizeMu  sync.Mutex

func (s *ClientStats) RecordSegmentSize(size int64) {
    s.segmentSizeMu.Lock()
    s.segmentSizes[s.segmentSizeIdx] = size
    s.segmentSizeIdx = (s.segmentSizeIdx + 1) % SegmentSizeRingSize
    s.segmentSizeMu.Unlock()
}
```

**Analysis:**
- **Frequency**: Medium (called per segment)
- **Complexity**: Medium (ring buffer with index)

**Conversion Strategy with sync.Pool:**
```go
// Use atomic.Value to swap entire ring buffer state
type segmentSizeState struct {
    sizes []int64
    idx   int
}

// Reset clears all fields to prepare for reuse from pool
func (s *segmentSizeState) Reset() {
    // Clear slice contents (zero out all elements)
    for i := range s.sizes {
        s.sizes[i] = 0
    }
    s.idx = 0
}

// Pool for reusing state structs (reduces GC pressure)
var segmentSizeStatePool = sync.Pool{
    New: func() interface{} {
        return &segmentSizeState{
            sizes: make([]int64, SegmentSizeRingSize),
        }
    },
}

segmentSizeState atomic.Value // *segmentSizeState

func (s *ClientStats) RecordSegmentSize(size int64) {
    // Load current state
    current := s.segmentSizeState.Load().(*segmentSizeState)

    // Get new state from pool (or create if pool empty)
    newState := segmentSizeStatePool.Get().(*segmentSizeState)

    // CRITICAL: Reset before use to ensure clean state
    // Even though we copy from current, reset ensures no stale data
    newState.Reset()

    // Copy current sizes into new state
    copy(newState.sizes, current.sizes)
    newState.idx = (current.idx + 1) % SegmentSizeRingSize
    newState.sizes[newState.idx] = size

    // Atomically swap
    oldState := s.segmentSizeState.Swap(newState).(*segmentSizeState)

    // CRITICAL: Reset old state before returning to pool
    // This ensures the next Get() returns a clean, ready-to-use struct
    oldState.Reset()

    // Put old state back in pool (will be reused on next call)
    // Safe to put back immediately because:
    // 1. Old state is immutable (readers only read, never modify)
    // 2. After Swap(), new readers get the new state from Load()
    // 3. Any existing readers with pointer to old state can continue safely
    // 4. This is copy-on-write semantics - we never modify old values
    segmentSizeStatePool.Put(oldState)
}
```

**Trade-offs:**
- ‚úÖ Lock-free
- ‚úÖ **Reduced GC pressure** (sync.Pool reuses structs)
- ‚úÖ Slice reuse (pooled structs have pre-allocated slices)
- ‚ö†Ô∏è Slightly more complex than mutex
- ‚ö†Ô∏è Need to initialize pool in NewClientStats()

**Pool Management:**
- Pool automatically manages allocation/deallocation
- Old structs are reused, reducing allocations by ~90%
- If pool is empty, `New()` function creates a new struct
- Pool naturally balances (grows when needed, shrinks when idle)
- **Reset() ensures clean state** - no stale data leaks between uses

**Lifetime Management Pattern:**
1. `Get()` from pool ‚Üí may have stale data
2. `Reset()` ‚Üí clears all fields to zero
3. Initialize with new values ‚Üí ready to use
4. `Swap()` ‚Üí atomically update
5. `Reset()` on old value ‚Üí clear before return
6. `Put()` back in pool ‚Üí ready for next `Get()`

**Recommendation**: ‚úÖ **CONVERT** - With sync.Pool and proper Reset(), GC pressure is minimal. Good candidate for conversion.

---

#### 1.5 `speedMu` - ‚≠ê‚≠ê MEDIUM

**Current Usage:**
```go
CurrentSpeed          float64
speedBelowThresholdAt time.Time
speedMu               sync.Mutex

func (s *ClientStats) UpdateSpeed(speed float64) {
    s.speedMu.Lock()
    s.CurrentSpeed = speed
    if speed > 0 && speed < StallThreshold {
        if s.speedBelowThresholdAt.IsZero() {
            s.speedBelowThresholdAt = time.Now()
        }
    } else {
        s.speedBelowThresholdAt = time.Time{}
    }
    s.speedMu.Unlock()
}
```

**Analysis:**
- **Frequency**: High (called on every progress update)
- **Complexity**: Medium (two fields, conditional logic)

**Conversion Strategy with sync.Pool:**
```go
type speedState struct {
    speed              float64
    belowThresholdAt   time.Time
}

// Reset clears all fields to prepare for reuse from pool
func (s *speedState) Reset() {
    s.speed = 0
    s.belowThresholdAt = time.Time{}
}

// Pool for reusing state structs (reduces GC pressure)
var speedStatePool = sync.Pool{
    New: func() interface{} {
        return &speedState{}
    },
}

speedState atomic.Value // *speedState

func (s *ClientStats) UpdateSpeed(speed float64) {
    // Load current state
    current := s.speedState.Load().(*speedState)

    // Get new state from pool (or create if pool empty)
    newState := speedStatePool.Get().(*speedState)

    // CRITICAL: Reset before use to ensure clean state
    newState.Reset()

    // Initialize new state
    newState.speed = speed
    if speed > 0 && speed < StallThreshold {
        if current.belowThresholdAt.IsZero() {
            newState.belowThresholdAt = time.Now()
        } else {
            newState.belowThresholdAt = current.belowThresholdAt
        }
    } else {
        newState.belowThresholdAt = time.Time{}
    }

    // Atomically swap
    oldState := s.speedState.Swap(newState).(*speedState)

    // CRITICAL: Reset old state before returning to pool
    // This ensures the next Get() returns a clean, ready-to-use struct
    oldState.Reset()

    // Put old state back in pool (will be reused on next call)
    speedStatePool.Put(oldState)
}

func (s *ClientStats) GetSpeed() float64 {
    state := s.speedState.Load().(*speedState)
    return state.speed
}

func (s *ClientStats) IsStalled() bool {
    state := s.speedState.Load().(*speedState)
    if state.belowThresholdAt.IsZero() {
        return false
    }
    return time.Since(state.belowThresholdAt) > StallDuration
}
```

**Trade-offs:**
- ‚úÖ Lock-free
- ‚úÖ **Reduced GC pressure** (sync.Pool reuses structs)
- ‚úÖ All reads are lock-free
- ‚ö†Ô∏è Slightly more complex than mutex
- ‚ö†Ô∏è Need to initialize pool in NewClientStats()

**Pool Management:**
- Pool automatically manages allocation/deallocation
- Old structs are reused, reducing allocations by ~90%
- Very small struct (16 bytes), so pool overhead is minimal
- **Reset() ensures clean state** - no stale data leaks between uses

**Lifetime Management Pattern:**
1. `Get()` from pool ‚Üí may have stale data
2. `Reset()` ‚Üí clears all fields to zero
3. Initialize with new values ‚Üí ready to use
4. `Swap()` ‚Üí atomically update
5. `Reset()` on old value ‚Üí clear before return
6. `Put()` back in pool ‚Üí ready for next `Get()`

**Recommendation**: ‚úÖ **CONVERT** - With sync.Pool and proper Reset(), GC pressure is minimal. High frequency makes this valuable.

---

#### 1.6 `driftMu` - ‚≠ê‚≠ê MEDIUM

**Current Usage:**
```go
LastPlaybackTime time.Duration
CurrentDrift     time.Duration
MaxDrift         time.Duration
driftMu          sync.Mutex

func (s *ClientStats) UpdateDrift(playbackTime time.Duration) {
    s.driftMu.Lock()
    s.LastPlaybackTime = playbackTime
    s.CurrentDrift = wallClockElapsed - playbackTime
    if s.CurrentDrift > s.MaxDrift {
        s.MaxDrift = s.CurrentDrift
    }
    s.driftMu.Unlock()
}
```

**Analysis:**
- **Frequency**: High (called on every progress update)
- **Complexity**: Medium (three fields, max calculation)

**Conversion Strategy with sync.Pool:**
```go
type driftState struct {
    lastPlaybackTime time.Duration
    currentDrift     time.Duration
    maxDrift         time.Duration
}

// Reset clears all fields to prepare for reuse from pool
func (d *driftState) Reset() {
    d.lastPlaybackTime = 0
    d.currentDrift = 0
    d.maxDrift = 0
}

// Pool for reusing state structs (reduces GC pressure)
var driftStatePool = sync.Pool{
    New: func() interface{} {
        return &driftState{}
    },
}

driftState atomic.Value // *driftState

func (s *ClientStats) UpdateDrift(playbackTime time.Duration) {
    wallClockElapsed := time.Since(s.StartTime)

    // Load current state
    current := s.driftState.Load().(*driftState)

    // Get new state from pool (or create if pool empty)
    newState := driftStatePool.Get().(*driftState)

    // CRITICAL: Reset before use to ensure clean state
    newState.Reset()

    // Initialize new state
    newState.lastPlaybackTime = playbackTime
    newState.currentDrift = wallClockElapsed - playbackTime

    // Calculate max drift
    if newState.currentDrift > current.maxDrift {
        newState.maxDrift = newState.currentDrift
    } else {
        newState.maxDrift = current.maxDrift
    }

    // Atomically swap
    oldState := s.driftState.Swap(newState).(*driftState)

    // CRITICAL: Reset old state before returning to pool
    // This ensures the next Get() returns a clean, ready-to-use struct
    oldState.Reset()

    // Put old state back in pool (will be reused on next call)
    driftStatePool.Put(oldState)
}

func (s *ClientStats) GetDrift() (current, max time.Duration) {
    state := s.driftState.Load().(*driftState)
    return state.currentDrift, state.maxDrift
}

func (s *ClientStats) HasHighDrift() bool {
    state := s.driftState.Load().(*driftState)
    return state.currentDrift > HighDriftThreshold
}
```

**Trade-offs:**
- ‚úÖ Lock-free
- ‚úÖ **Reduced GC pressure** (sync.Pool reuses structs)
- ‚úÖ All reads are lock-free
- ‚ö†Ô∏è Slightly more complex than mutex
- ‚ö†Ô∏è Need to initialize pool in NewClientStats()

**Pool Management:**
- Pool automatically manages allocation/deallocation
- Old structs are reused, reducing allocations by ~90%
- Very small struct (24 bytes), so pool overhead is minimal
- **Reset() ensures clean state** - no stale data leaks between uses

**Lifetime Management Pattern:**
1. `Get()` from pool ‚Üí may have stale data
2. `Reset()` ‚Üí clears all fields to zero
3. Initialize with new values ‚Üí ready to use
4. `Swap()` ‚Üí atomically update
5. `Reset()` on old value ‚Üí clear before return
6. `Put()` back in pool ‚Üí ready for next `Get()`

**Recommendation**: ‚úÖ **CONVERT** - With sync.Pool and proper Reset(), GC pressure is minimal. High frequency makes this valuable.

---

#### 1.7 `peakDropMu` - ‚≠ê EASY

**Current Usage:**
```go
PeakDropRate float64
peakDropMu   sync.Mutex

func (s *ClientStats) UpdatePeakDropRate() {
    currentRate := s.CurrentDropRate()
    s.peakDropMu.Lock()
    if currentRate > s.PeakDropRate {
        s.PeakDropRate = currentRate
    }
    s.peakDropMu.Unlock()
}

func (s *ClientStats) GetPeakDropRate() float64 {
    s.peakDropMu.Lock()
    defer s.peakDropMu.Unlock()
    return s.PeakDropRate
}
```

**Analysis:**
- **Frequency**: Medium (called periodically, not per-event)
- **Complexity**: Low (single float64, max operation)

**Conversion Strategy:**
```go
peakDropRate atomic.Uint64 // Store as uint64 bits

func (s *ClientStats) UpdatePeakDropRate() {
    currentRate := s.CurrentDropRate()
    for {
        oldBits := s.peakDropRate.Load()
        oldRate := math.Float64frombits(oldBits)
        if currentRate <= oldRate {
            break // No update needed
        }
        if s.peakDropRate.CompareAndSwap(oldBits, math.Float64bits(currentRate)) {
            break // Successfully updated
        }
        // Retry on CAS failure
    }
}

func (s *ClientStats) GetPeakDropRate() float64 {
    return math.Float64frombits(s.peakDropRate.Load())
}
```

**Benefits:**
- ‚úÖ Lock-free
- ‚úÖ CAS loop for max operation (standard pattern)

**Difficulty**: ‚≠ê Easy (15 LOC changes)

**Recommendation**: ‚úÖ **CONVERT** - Easy, good practice

---

### 2. `internal/stats/aggregator.go` - MEDIUM PRIORITY

#### 2.1 `snapshotMu` - ‚≠ê‚≠ê MEDIUM (Already done for debug stats!)

**Current Usage:**
```go
snapshotMu   sync.Mutex
prevSnapshot *rateSnapshot

// In Aggregate():
a.snapshotMu.Lock()
prevSnapshot := a.prevSnapshot
a.snapshotMu.Unlock()
// ... calculate rates ...
a.snapshotMu.Lock()
a.prevSnapshot = &rateSnapshot{...}
a.snapshotMu.Unlock()
```

**Analysis:**
- **Frequency**: Medium (called per-TUI tick, ~2/sec)
- **Pattern**: Identical to `debugSnapshotMu` we just converted!

**Conversion Strategy:**
```go
prevSnapshot atomic.Value // *rateSnapshot

// In Aggregate():
prevSnapshotPtr := a.prevSnapshot.Load()
if prevSnapshotPtr != nil {
    prevSnapshot := prevSnapshotPtr.(*rateSnapshot)
    // ... calculate rates ...
}
newSnapshot := &rateSnapshot{...}
a.prevSnapshot.Store(newSnapshot)
```

**Benefits:**
- ‚úÖ Same pattern as debug stats (proven approach)
- ‚úÖ Lock-free reads/writes

**Difficulty**: ‚≠ê‚≠ê Medium (5 LOC changes, but proven pattern)

**Recommendation**: ‚úÖ **CONVERT** - Same pattern as Phase 7.4.1

---

#### 2.2 `peakDropRateMu` - ‚≠ê EASY

**Current Usage:**
```go
peakDropRate   float64
peakDropRateMu sync.Mutex

func (a *StatsAggregator) GetPeakDropRate() float64 {
    a.peakDropRateMu.Lock()
    defer a.peakDropRateMu.Unlock()
    return a.peakDropRate
}
```

**Analysis:**
- **Frequency**: Low (called rarely)
- **Complexity**: Low (single float64, max operation)

**Conversion Strategy:**
Same as `client_stats.go` peakDropMu (CAS loop with atomic.Uint64)

**Recommendation**: ‚úÖ **CONVERT** - Easy, consistent with client_stats

---

#### 2.3 `mu` (RWMutex for clients map) - ‚ùå KEEP

**Current Usage:**
```go
mu      sync.RWMutex
clients map[int]*ClientStats
```

**Analysis:**
- **Frequency**: Medium (called on client add/remove, aggregation)
- **Complexity**: High (map operations can't be atomic)
- **Alternatives**: `sync.Map` (but adds complexity, may not be faster)

**Recommendation**: ‚ùå **KEEP** - Map operations require mutex, RWMutex is already optimal

---

### 3. `internal/orchestrator/client_manager.go` - LOW PRIORITY

#### 3.1 `mu` (RWMutex for supervisors map) - ‚ùå KEEP

**Current Usage:**
```go
mu          sync.RWMutex
supervisors map[int]*supervisor.Supervisor
```

**Analysis:**
- **Frequency**: Low (called on client start/stop, not per-event)
- **Complexity**: High (map operations)

**Recommendation**: ‚ùå **KEEP** - Low contention, RWMutex is optimal

---

### 4. `internal/supervisor/supervisor.go` - LOW PRIORITY

#### 4.1 `cmdMu` - ‚ùå KEEP

**Current Usage:**
```go
cmd   *exec.Cmd
cmdMu sync.Mutex

// Used to protect cmd pointer during start/stop
```

**Analysis:**
- **Frequency**: Very Low (only on process start/stop)
- **Complexity**: Low (single pointer)
- **Why Keep**: Process lifecycle operations are rare, mutex overhead is negligible

**Recommendation**: ‚ùå **KEEP** - Very low frequency, not worth optimizing

---

### 5. `internal/parser/debug_events.go` - MEDIUM PRIORITY

#### 5.1 `mu` - ‚ùå KEEP

**Current Usage:**
```go
mu sync.Mutex

// Protects multiple fields:
// - segmentDownloads map
// - playlistRefreshes map
// - tcpConnects map
// - segmentWallTimes slice
// - etc.
```

**Analysis:**
- **Frequency**: High (called per debug event)
- **Complexity**: Very High (multiple maps, slices, complex state)

**Recommendation**: ‚ùå **KEEP** - Too complex, mutex is necessary

---

### 6. `internal/parser/socket_reader.go` - LOW PRIORITY

#### 6.1 `connMu` - ‚ùå KEEP

**Current Usage:**
```go
conn   net.Conn
connMu sync.Mutex

// Protects conn during Accept/Close operations
```

**Analysis:**
- **Frequency**: Very Low (only on connection setup/teardown)
- **Complexity**: Low (single pointer)
- **Why Keep**: Network operations are rare, mutex overhead is negligible

**Recommendation**: ‚ùå **KEEP** - Very low frequency, not worth optimizing

---

### 7. `internal/parser/hls_events.go` - LOW PRIORITY

#### 7.1 `mu` - ‚ùå KEEP

**Current Usage:**
```go
mu sync.Mutex

// Protects latency tracking (slice operations)
```

**Analysis:**
- **Frequency**: Medium (called per HLS event)
- **Complexity**: Medium (slice append operations)
- **Note**: This parser is being replaced by DebugEventParser (Phase 7)

**Recommendation**: ‚ùå **KEEP** - Legacy code, will be removed

---

### 8. `internal/parser/progress.go` - LOW PRIORITY

#### 8.1 `mu` - ‚ùå KEEP

**Current Usage:**
```go
mu      sync.Mutex
current *ProgressUpdate

// Protects current progress update pointer
```

**Analysis:**
- **Frequency**: High (called per progress line)
- **Complexity**: Low (single pointer)
- **Why Keep**: Could use atomic.Value, but:
  - Progress updates are already fast
  - Mutex contention is low (one per client)
  - Not a bottleneck

**Recommendation**: ‚ùå **KEEP** - Low contention, not a bottleneck

---

### 9. `internal/metrics/collector.go` - LOW PRIORITY

#### 9.1 `mu` - ‚ùå KEEP

**Current Usage:**
```go
mu               sync.Mutex
prevManifestReqs int64
prevSegmentReqs  int64
prevInitReqs     int64
```

**Analysis:**
- **Frequency**: Low (called per metrics scrape, ~1/sec)
- **Complexity**: Low (three int64 values)
- **Why Keep**: Very low frequency, mutex overhead is negligible

**Recommendation**: ‚ùå **KEEP** - Very low frequency, not worth optimizing

---

### 10. `internal/logging/handler.go` - LOW PRIORITY

#### 10.1 `mu` - ‚ùå KEEP

**Current Usage:**
```go
mu     sync.Mutex
buffer []string
bufIdx int
```

**Analysis:**
- **Frequency**: Low (logging operations)
- **Complexity**: Medium (ring buffer)
- **Why Keep**: Logging is not performance-critical

**Recommendation**: ‚ùå **KEEP** - Logging is not a bottleneck

---

## Summary Table

| File | Mutex | Priority | Difficulty | Recommendation | Impact |
|------|-------|----------|------------|----------------|--------|
| `client_stats.go` | `bytesMu` | ‚ö° High | ‚≠ê Easy | ‚úÖ **CONVERT** | High |
| `client_stats.go` | `httpErrorsMu` | Medium | ‚ùå N/A | ‚ùå Keep | Low |
| `client_stats.go` | `inferredLatencyMu` | High | üóëÔ∏è Remove | üóëÔ∏è **REMOVE** | N/A |
| `client_stats.go` | `segmentSizeMu` | Medium | ‚≠ê‚≠ê Medium | ‚úÖ **CONVERT** (with sync.Pool) | Medium |
| `client_stats.go` | `speedMu` | High | ‚≠ê‚≠ê Medium | ‚úÖ **CONVERT** (with sync.Pool) | High |
| `client_stats.go` | `driftMu` | High | ‚≠ê‚≠ê Medium | ‚úÖ **CONVERT** (with sync.Pool) | High |
| `client_stats.go` | `peakDropMu` | Medium | ‚≠ê Easy | ‚úÖ **CONVERT** | Low |
| `aggregator.go` | `snapshotMu` | Medium | ‚≠ê‚≠ê Medium | ‚úÖ **CONVERT** | Medium |
| `aggregator.go` | `peakDropRateMu` | Low | ‚≠ê Easy | ‚úÖ **CONVERT** | Low |
| `aggregator.go` | `mu` (RWMutex) | Medium | ‚ùå N/A | ‚ùå Keep | N/A |
| `client_manager.go` | `mu` (RWMutex) | Low | ‚ùå N/A | ‚ùå Keep | N/A |
| `supervisor.go` | `cmdMu` | Low | ‚ùå N/A | ‚ùå Keep | N/A |
| `debug_events.go` | `mu` | High | ‚ùå N/A | ‚ùå Keep | N/A |
| `socket_reader.go` | `connMu` | Low | ‚ùå N/A | ‚ùå Keep | N/A |
| `hls_events.go` | `mu` | Low | ‚ùå N/A | ‚ùå Keep | N/A |
| `progress.go` | `mu` | Low | ‚ùå N/A | ‚ùå Keep | N/A |
| `metrics/collector.go` | `mu` | Low | ‚ùå N/A | ‚ùå Keep | N/A |
| `logging/handler.go` | `mu` | Low | ‚ùå N/A | ‚ùå Keep | N/A |

---

## Recommended Implementation Order

### Phase 1: High-Value Easy Wins (Immediate)

1. ‚úÖ **`client_stats.go:bytesMu`** - High frequency, easy conversion
2. ‚úÖ **`client_stats.go:peakDropMu`** - Easy, good practice
3. ‚úÖ **`aggregator.go:snapshotMu`** - Same pattern as debug stats (proven)
4. ‚úÖ **`aggregator.go:peakDropRateMu`** - Easy, consistent

**Estimated Effort**: 1-2 hours
**Estimated Impact**: Medium-High (reduces contention in hot paths)

### Phase 2: High-Value Complex State (COMPLETED ‚úÖ)

5. ‚úÖ **`client_stats.go:speedMu`** - Converted to individual atomics (`atomic.Uint64` for speed, `atomic.Value` for timestamp)
6. ‚úÖ **`client_stats.go:driftMu`** - Converted to individual atomics (`atomic.Int64` for each duration field)
7. ‚úÖ **`client_stats.go:segmentSizeMu`** - Converted to atomic index + shared slice

**Implementation**: Used individual atomics instead of sync.Pool pattern
- **Rationale**: Simpler, no race conditions, no allocations, better performance
- **Trade-off**: Brief out-of-sync between fields is acceptable for these metrics
- **Status**: ‚úÖ All tests pass with race detection

**Estimated Effort**: 2-3 hours (completed)
**Estimated Impact**: High (reduces contention in hot paths, zero GC pressure, no race conditions)

---

## Implementation Guidelines

### For Easy Conversions (atomic.Int64, etc.)

1. Replace mutex with atomic type
2. Use `Load()` for reads
3. Use `Store()` for writes
4. Use `Add()` for increments
5. Use CAS loop for max/min operations

### For Medium Conversions: Individual Atomics (Recommended ‚úÖ)

**Approach**: Use individual atomic fields instead of struct swap pattern.

**Benefits**:
- ‚úÖ No race conditions (each field is independent)
- ‚úÖ No allocations (zero GC pressure)
- ‚úÖ Simpler code (no Reset() methods, no pool management)
- ‚úÖ Better performance (no struct copying)

**Implementation Pattern**:
```go
// Instead of struct swap:
// speedState atomic.Value // *speedState

// Use individual atomics:
speed            atomic.Uint64 // math.Float64bits(speed)
belowThresholdAt atomic.Value  // time.Time

func (s *ClientStats) UpdateSpeed(speed float64) {
    // Load current speed to check threshold crossing
    currentSpeed := math.Float64frombits(s.speed.Load())

    // Update speed atomically
    s.speed.Store(math.Float64bits(speed))

    // Update timestamp based on transition
    if speed > 0 && speed < StallThreshold {
        if currentSpeed >= StallThreshold {
            s.belowThresholdAt.Store(time.Now())
        }
    } else {
        s.belowThresholdAt.Store(time.Time{})
    }
}
```

**Trade-offs**:
- Brief out-of-sync between fields is acceptable for these metrics
- Update order matters (update primary field first, then derived fields)

**For time.Duration fields**: Use `atomic.Int64` (store as nanoseconds)
```go
lastPlaybackTime atomic.Int64 // time.Duration as nanoseconds
currentDrift     atomic.Int64 // time.Duration as nanoseconds
maxDrift         atomic.Int64 // time.Duration as nanoseconds

func (s *ClientStats) UpdateDrift(outTimeUS int64) {
    playbackTime := time.Duration(outTimeUS) * time.Microsecond
    current := time.Since(s.StartTime) - playbackTime

    s.lastPlaybackTime.Store(int64(playbackTime))
    s.currentDrift.Store(int64(current))

    // Update max using CAS loop
    for {
        oldMax := s.maxDrift.Load()
        if int64(current) <= oldMax {
            break
        }
        if s.maxDrift.CompareAndSwap(oldMax, int64(current)) {
            break
        }
    }
}
```

**For ring buffers**: Use atomic index + shared slice
```go
segmentSizes   []int64      // Shared slice (read-only after init)
segmentSizeIdx atomic.Int64 // Atomic index

func (s *ClientStats) RecordSegmentSize(size int64) {
    oldIdx := s.segmentSizeIdx.Load()
    newIdx := (oldIdx + 1) % SegmentSizeRingSize
    s.segmentSizeIdx.Store(newIdx)
    s.segmentSizes[newIdx] = size // Brief inconsistency acceptable
}
```

### Alternative: atomic.Value with sync.Pool (Not Recommended ‚ùå)

**Note**: This approach was initially considered but replaced with individual atomics due to:
- Race conditions with Reset() timing
- Complexity of object lifetime management
- No performance benefit over individual atomics
- See `docs/ATOMIC_POOL_RACE_ANALYSIS.md` for details

### Testing Requirements

1. **Race detection**: `go test -race`
2. **Concurrent access tests**: Multiple goroutines calling the method
3. **Correctness tests**: Verify values are correct under concurrency
4. **Performance benchmarks**: Compare before/after (optional)

---

## Expected Benefits

### High-Value Conversions

- **`bytesMu`**: Eliminates lock contention on every progress update (high frequency)
- **`snapshotMu`**: Eliminates lock contention on every TUI tick (~2/sec per client)

### High-Value Conversions with sync.Pool

- **`speedMu`, `driftMu`, `segmentSizeMu`**: High frequency operations, sync.Pool eliminates GC pressure concerns
- **`peakDropMu`**: Reduces lock contention, good practice

### Overall Impact

- **<1000 clients**: Negligible difference (mutexes are already fast)
- **1000-10,000 clients**: Small improvement (smoother operations)
- **>10,000 clients**: Significant improvement (no lock contention bottlenecks)

---

## Conclusion

**Recommended Actions:**
1. ‚úÖ **Convert 4 easy wins** (bytesMu, peakDropMu x2, snapshotMu) - High value, low risk - **COMPLETED**
2. ‚úÖ **Convert 3 high-frequency complex state** (speedMu, driftMu, segmentSizeMu) - High value, zero GC pressure - **COMPLETED**
3. ‚ùå **Keep all others** - Either too complex or too low frequency

**Total Estimated Effort**:
- Phase 1: 1-2 hours (4 easy wins) - **COMPLETED**
- Phase 2: 2-3 hours (3 complex state) - **COMPLETED** (using individual atomics instead of sync.Pool)
- **Total: 3-5 hours** - **COMPLETED**

**Total Estimated Impact**: High (reduces contention in all hot paths, zero GC overhead, no race conditions)

**Implementation Notes:**
- Phase 2 was implemented using **individual atomics** instead of sync.Pool pattern
- **Rationale**: Simpler, no race conditions, no allocations, better performance
- **Trade-off**: Brief out-of-sync between fields is acceptable for these metrics
- **Status**: ‚úÖ All tests pass with race detection (`go test -race`)
- See `docs/ATOMIC_POOL_RACE_ANALYSIS.md` for analysis of why individual atomics were chosen over sync.Pool

**Remaining Mutexes (Intentionally Kept):**
- `httpErrorsMu` - Map operations require mutex
- `segmentWallTimeDigestMu` - TDigest is not thread-safe, requires mutex
- `mu` in various parsers - Complex state, low contention
- `connMu`, `cmdMu` - Very low frequency, not worth optimizing
- All test mutexes - Test code only
